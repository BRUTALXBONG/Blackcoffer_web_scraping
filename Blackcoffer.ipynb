{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8b98a9-ee04-424e-ac30-4b2b8cdc7353",
   "metadata": {},
   "source": [
    "##### MATERIALS - https://drive.google.com/drive/folders/1ltdsXAS_zaZ3hI-q9eze_QCzHciyYAJY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26229843-2f97-43ff-91fa-b5d6eac6af80",
   "metadata": {},
   "source": [
    "### DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db24f1b-c434-4185-bd11-15126ca7cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for URL_ID blackassign0001\n",
      "Extracted text for URL_ID blackassign0002\n",
      "Extracted text for URL_ID blackassign0003\n",
      "Extracted text for URL_ID blackassign0004\n",
      "Extracted text for URL_ID blackassign0005\n",
      "Extracted text for URL_ID blackassign0006\n",
      "Extracted text for URL_ID blackassign0007\n",
      "Extracted text for URL_ID blackassign0008\n",
      "Extracted text for URL_ID blackassign0009\n",
      "Extracted text for URL_ID blackassign0010\n",
      "Extracted text for URL_ID blackassign0011\n",
      "Extracted text for URL_ID blackassign0012\n",
      "Extracted text for URL_ID blackassign0013\n",
      "Extracted text for URL_ID blackassign0014\n",
      "Extracted text for URL_ID blackassign0015\n",
      "Extracted text for URL_ID blackassign0016\n",
      "Extracted text for URL_ID blackassign0017\n",
      "Extracted text for URL_ID blackassign0018\n",
      "Extracted text for URL_ID blackassign0019\n",
      "Extracted text for URL_ID blackassign0020\n",
      "Extracted text for URL_ID blackassign0021\n",
      "Extracted text for URL_ID blackassign0022\n",
      "Extracted text for URL_ID blackassign0023\n",
      "Extracted text for URL_ID blackassign0024\n",
      "Extracted text for URL_ID blackassign0025\n",
      "Extracted text for URL_ID blackassign0026\n",
      "Extracted text for URL_ID blackassign0027\n",
      "Extracted text for URL_ID blackassign0028\n",
      "Extracted text for URL_ID blackassign0029\n",
      "Extracted text for URL_ID blackassign0030\n",
      "Extracted text for URL_ID blackassign0031\n",
      "Extracted text for URL_ID blackassign0032\n",
      "Extracted text for URL_ID blackassign0033\n",
      "Extracted text for URL_ID blackassign0034\n",
      "Extracted text for URL_ID blackassign0035\n",
      "Extracted text for URL_ID blackassign0036\n",
      "Extracted text for URL_ID blackassign0037\n",
      "Extracted text for URL_ID blackassign0038\n",
      "Extracted text for URL_ID blackassign0039\n",
      "Extracted text for URL_ID blackassign0040\n",
      "Extracted text for URL_ID blackassign0041\n",
      "Extracted text for URL_ID blackassign0042\n",
      "Extracted text for URL_ID blackassign0043\n",
      "Extracted text for URL_ID blackassign0044\n",
      "Extracted text for URL_ID blackassign0045\n",
      "Extracted text for URL_ID blackassign0046\n",
      "Extracted text for URL_ID blackassign0047\n",
      "Extracted text for URL_ID blackassign0048\n",
      "Extracted text for URL_ID blackassign0049\n",
      "Extracted text for URL_ID blackassign0050\n",
      "Extracted text for URL_ID blackassign0051\n",
      "Extracted text for URL_ID blackassign0052\n",
      "Extracted text for URL_ID blackassign0053\n",
      "Extracted text for URL_ID blackassign0054\n",
      "Extracted text for URL_ID blackassign0055\n",
      "Extracted text for URL_ID blackassign0056\n",
      "Extracted text for URL_ID blackassign0057\n",
      "Extracted text for URL_ID blackassign0058\n",
      "Extracted text for URL_ID blackassign0059\n",
      "Extracted text for URL_ID blackassign0060\n",
      "Extracted text for URL_ID blackassign0061\n",
      "Extracted text for URL_ID blackassign0062\n",
      "Extracted text for URL_ID blackassign0063\n",
      "Extracted text for URL_ID blackassign0064\n",
      "Extracted text for URL_ID blackassign0065\n",
      "Extracted text for URL_ID blackassign0066\n",
      "Extracted text for URL_ID blackassign0067\n",
      "Extracted text for URL_ID blackassign0068\n",
      "Extracted text for URL_ID blackassign0069\n",
      "Extracted text for URL_ID blackassign0070\n",
      "Extracted text for URL_ID blackassign0071\n",
      "Extracted text for URL_ID blackassign0072\n",
      "Extracted text for URL_ID blackassign0073\n",
      "Extracted text for URL_ID blackassign0074\n",
      "Extracted text for URL_ID blackassign0075\n",
      "Extracted text for URL_ID blackassign0076\n",
      "Extracted text for URL_ID blackassign0077\n",
      "Extracted text for URL_ID blackassign0078\n",
      "Extracted text for URL_ID blackassign0079\n",
      "Extracted text for URL_ID blackassign0080\n",
      "Extracted text for URL_ID blackassign0081\n",
      "Extracted text for URL_ID blackassign0082\n",
      "Extracted text for URL_ID blackassign0083\n",
      "Extracted text for URL_ID blackassign0084\n",
      "Extracted text for URL_ID blackassign0085\n",
      "Extracted text for URL_ID blackassign0086\n",
      "Extracted text for URL_ID blackassign0087\n",
      "Extracted text for URL_ID blackassign0088\n",
      "Extracted text for URL_ID blackassign0089\n",
      "Extracted text for URL_ID blackassign0090\n",
      "Extracted text for URL_ID blackassign0091\n",
      "Extracted text for URL_ID blackassign0092\n",
      "Extracted text for URL_ID blackassign0093\n",
      "Extracted text for URL_ID blackassign0094\n",
      "Extracted text for URL_ID blackassign0095\n",
      "Extracted text for URL_ID blackassign0096\n",
      "Extracted text for URL_ID blackassign0097\n",
      "Extracted text for URL_ID blackassign0098\n",
      "Extracted text for URL_ID blackassign0099\n",
      "Extracted text for URL_ID blackassign0100\n",
      "Extraction completed. Output saved to Output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read URLs from Excel file\n",
    "input_file = r\"C:\\Users\\User\\Downloads\\Input.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract article text from URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract article text\n",
    "        article_text = \"\"\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.get_text() + \"\\n\"\n",
    "        return article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a new column to store extracted text\n",
    "df['Extracted_Text'] = \"\"\n",
    "\n",
    "# Iterate over each URL in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "\n",
    "    # Extract article text\n",
    "    extracted_text = extract_article_text(url)\n",
    "\n",
    "    if extracted_text:\n",
    "        # Update the corresponding row in the DataFrame with the extracted text\n",
    "        df.at[index, 'Extracted_Text'] = extracted_text\n",
    "        print(f\"Extracted text for URL_ID {row['URL_ID']}\")\n",
    "    else:\n",
    "        print(f\"Skipping URL_ID {row['URL_ID']}. Could not extract article.\")\n",
    "\n",
    "# Save the modified DataFrame to Excel\n",
    "output_file = \"Output.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Extraction completed. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0d3e1-7e02-492d-8c50-a2bac7b3603a",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc8feca-08a2-442c-818d-a9a70a470df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grafana Dashboard to visualize and analyze sensors’ data\\nMVP for a software that analyses content from audio (Pharma-based)\\nData Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database\\nText Summarizing Tool to scrape and summarize pubmed medical papers\\xa0\\nMethodology for ETL Discovery Tool using LLMA, OpenAI, Langchain\\nMethodology for database discovery tool using openai, LLMA, Langchain\\nChatbot using VoiceFlow\\nHow To Secure (SSL) Nginx with Let’s Encrypt on Ubuntu (Cloud VM, GCP, AWS, Azure, Linode) and Add Domain\\nRising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.\\nRising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future\\nInternet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways\\nRise of Cybercrime and its Effect in upcoming Future\\nAI/ML and Predictive Modeling\\nSolution for Contact Centre Problems\\nHow to Setup Custom Domain for Google App Engine Application?\\nCode Review Checklist\\nWe have seen a huge development and dependence of people on technology in recent years. We have also seen the development of AI and ChatGPT in recent years. So it is a normal thing that we will become fully dependent on technology by 2040. Information technology will be a major power for all the developing nations. As a member of a developing nation, India is rapidly growing its IT base. It has also grown some IT cities which will be the major control centres for Information technology by 2040.\\nRising IT cities\\nKolkata:- Kolkata in West Bengal is an emerging major IT hub. The new Kolkata i.e. Saltlake Sector\\xa0 5, New town, Rajarhat area of Kolkata is a major IT hub. The government is giving the software companies land at almost free of cost to set up the companies there. Many large companies like Google, Microsoft, IBM, Infosys and others have set up their companies here. Kolkata has a market base of billions of dollars and is doing a great job of boosting the national economy.\\nImpact on Economy\\nThere is a huge impact of the rising IT cities on our economy. Some of the effects are-\\nImpact on Environment\\nThe rising IT cities will create a huge impact on the environment, the maximum of which will be harmful effects. The impact of rising IT cities on the environment is-\\nImpact on infrastructure\\nThere are many contributions of the IT cities on infrastructure.\\xa0 They are-\\nImpact on city life\\nWith the growth of IT cities, more people will get jobs and will earn more. So the purchasing power of the people will increase. People will lead a better lifestyle. They will buy things of good brand value. The tastes and preferences of people will also change. The human development index is going to increase. People will buy good quality food and good quality cars. So the food, automobile and many other industries are going to increase. So there will be a huge impact on city life by 2040.\\nWe provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.\\nContact us: hello@blackcoffer.com\\n© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_excel(r\"C:\\Users\\User\\Downloads\\Output.xlsx\")\n",
    "data['Extracted_Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b72596a-7edd-4789-bfb0-61effa95348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "                                      Extracted_Text  \n",
       "0  Grafana Dashboard to visualize and analyze sen...  \n",
       "1  Grafana Dashboard to visualize and analyze sen...  \n",
       "2  Grafana Dashboard to visualize and analyze sen...  \n",
       "3  Grafana Dashboard to visualize and analyze sen...  \n",
       "4  Grafana Dashboard to visualize and analyze sen...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34cc9e59-b73e-4e90-a864-fdce114959f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>hub content brand modeling ie aiml specialist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>taken impossible aiml skill andor favorable in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>aiml robust openai scrape vulnerabilities capa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>elections reduce aiml generalised behaviour fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>players decadesold aiml remain enrich robust m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "                                      Extracted_Text  \n",
       "0  hub content brand modeling ie aiml specialist ...  \n",
       "1  taken impossible aiml skill andor favorable in...  \n",
       "2  aiml robust openai scrape vulnerabilities capa...  \n",
       "3  elections reduce aiml generalised behaviour fo...  \n",
       "4  players decadesold aiml remain enrich robust m...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (stop words)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data (replace \"input_file.xlsx\" with your actual file path)\n",
    "data = pd.read_excel(r\"C:\\Users\\User\\Downloads\\Output.xlsx\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and symbols\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Remove duplicate words\n",
    "    words = list(set(words))\n",
    "    # Join words back into text\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply text cleaning function to the 'Extracted_Text' column\n",
    "data['Extracted_Text'] = data['Extracted_Text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520869fa-cd26-43d3-9f2d-c77409d261f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hub content brand modeling ie aiml specialist ...\n",
       "1    taken impossible aiml skill andor favorable in...\n",
       "2    aiml robust openai scrape vulnerabilities capa...\n",
       "3    elections reduce aiml generalised behaviour fo...\n",
       "4    players decadesold aiml remain enrich robust m...\n",
       "Name: Extracted_Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Extracted_Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f379fe-e990-4012-9e02-39903b42a05d",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9064af04-abfa-4655-9e01-ffd2c5228969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted_Text</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>Subjectivity_Score</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_of_Complex_Words</th>\n",
       "      <th>FOG_Index</th>\n",
       "      <th>Avg_Words_Per_Sentence</th>\n",
       "      <th>Syllables_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "      <th>Avg_Word_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>hub content brand modeling ie aiml specialist ...</td>\n",
       "      <td>11</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>208.0</td>\n",
       "      <td>34.615385</td>\n",
       "      <td>97.046154</td>\n",
       "      <td>208.0</td>\n",
       "      <td>2.245192</td>\n",
       "      <td>1</td>\n",
       "      <td>6.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>taken impossible aiml skill andor favorable in...</td>\n",
       "      <td>46</td>\n",
       "      <td>-25</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>0.032659</td>\n",
       "      <td>621.0</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>264.696296</td>\n",
       "      <td>621.0</td>\n",
       "      <td>2.400966</td>\n",
       "      <td>1</td>\n",
       "      <td>7.450886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>aiml robust openai scrape vulnerabilities capa...</td>\n",
       "      <td>33</td>\n",
       "      <td>-19</td>\n",
       "      <td>3.714285</td>\n",
       "      <td>0.029979</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48.672566</td>\n",
       "      <td>200.269027</td>\n",
       "      <td>452.0</td>\n",
       "      <td>2.581858</td>\n",
       "      <td>1</td>\n",
       "      <td>7.809735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>elections reduce aiml generalised behaviour fo...</td>\n",
       "      <td>33</td>\n",
       "      <td>-64</td>\n",
       "      <td>-3.129032</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>514.0</td>\n",
       "      <td>43.579767</td>\n",
       "      <td>223.031907</td>\n",
       "      <td>514.0</td>\n",
       "      <td>2.503891</td>\n",
       "      <td>1</td>\n",
       "      <td>7.776265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>players decadesold aiml remain enrich robust m...</td>\n",
       "      <td>25</td>\n",
       "      <td>-10</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>381.0</td>\n",
       "      <td>42.257218</td>\n",
       "      <td>169.302887</td>\n",
       "      <td>381.0</td>\n",
       "      <td>2.433071</td>\n",
       "      <td>1</td>\n",
       "      <td>7.388451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "                                      Extracted_Text  Positive_Score  \\\n",
       "0  hub content brand modeling ie aiml specialist ...              11   \n",
       "1  taken impossible aiml skill andor favorable in...              46   \n",
       "2  aiml robust openai scrape vulnerabilities capa...              33   \n",
       "3  elections reduce aiml generalised behaviour fo...              33   \n",
       "4  players decadesold aiml remain enrich robust m...              25   \n",
       "\n",
       "   Negative_Score  Polarity_Score  Subjectivity_Score  Avg_Sentence_Length  \\\n",
       "0              -3        1.750000            0.037037                208.0   \n",
       "1             -25        3.380952            0.032659                621.0   \n",
       "2             -19        3.714285            0.029979                452.0   \n",
       "3             -64       -3.129032           -0.058491                514.0   \n",
       "4             -10        2.333333            0.037879                381.0   \n",
       "\n",
       "   Percentage_of_Complex_Words   FOG_Index  Avg_Words_Per_Sentence  \\\n",
       "0                    34.615385   97.046154                   208.0   \n",
       "1                    40.740741  264.696296                   621.0   \n",
       "2                    48.672566  200.269027                   452.0   \n",
       "3                    43.579767  223.031907                   514.0   \n",
       "4                    42.257218  169.302887                   381.0   \n",
       "\n",
       "   Syllables_Per_Word  Personal_Pronouns  Avg_Word_Length  \n",
       "0            2.245192                  1         6.701923  \n",
       "1            2.400966                  1         7.450886  \n",
       "2            2.581858                  1         7.809735  \n",
       "3            2.503891                  1         7.776265  \n",
       "4            2.433071                  1         7.388451  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources (CMU Pronouncing Dictionary)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load positive and negative words from Master Dictionary\n",
    "def load_master_dictionary(directory):\n",
    "    positive_words = set()\n",
    "    negative_words = set()\n",
    "\n",
    "    # Load positive words\n",
    "    with open(os.path.join(directory, \"positive-words.txt\"), \"r\") as file:\n",
    "        for line in file:\n",
    "            positive_words.add(line.strip())\n",
    "\n",
    "    # Load negative words\n",
    "    with open(os.path.join(directory, \"negative-words.txt\"), \"r\") as file:\n",
    "        for line in file:\n",
    "            negative_words.add(line.strip())\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "# Function to calculate positive and negative scores for a given text\n",
    "def calculate_scores(text, positive_words, negative_words):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Calculate positive and negative scores\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words) * -1\n",
    "\n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Function to calculate Polarity Score\n",
    "def calculate_polarity_score(pos_score, neg_score):\n",
    "    return (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "\n",
    "# Function to calculate Subjectivity Score\n",
    "def calculate_subjectivity_score(pos_score, neg_score, total_words):\n",
    "    return (pos_score + neg_score) / (total_words + 0.000001)\n",
    "\n",
    "# Function to analyze readability and calculate additional derived variables\n",
    "def analyze_readability(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "\n",
    "    # Calculate derived variables\n",
    "    word_count = len(words)\n",
    "    avg_sentence_length = word_count / num_sentences\n",
    "    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = word_count / num_sentences\n",
    "    syllables_per_word = sum(count_syllables(word) for word in words) / word_count\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'])\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count\n",
    "\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, syllables_per_word, personal_pronouns, avg_word_length\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    # Implementation of count_syllables function (using CMU Pronouncing Dictionary or other method)\n",
    "    # This function should return the number of syllables in the word\n",
    "    # For demonstration purposes, let's assume a simple implementation using vowel counting\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    syllable_count = 0\n",
    "    previous_char_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        if char in vowels and not previous_char_was_vowel:\n",
    "            syllable_count += 1\n",
    "            previous_char_was_vowel = True\n",
    "        elif char not in vowels:\n",
    "            previous_char_was_vowel = False\n",
    "            \n",
    "    # Handle exceptions like words ending with \"es\", \"ed\"\n",
    "    if word.endswith(('es', 'ed')):\n",
    "        syllable_count -= 1\n",
    "    \n",
    "    # Ensure syllable count is at least 1\n",
    "    return max(syllable_count, 1)\n",
    "\n",
    "\n",
    "# Load positive and negative words from Master Dictionary\n",
    "    master_dictionary_dir = \"C:\\\\Users\\\\User\\\\Downloads\\\\MasterDictionary\"  # Replace with the correct directory path\n",
    "    positive_words, negative_words = load_master_dictionary(master_dictionary_dir)\n",
    "\n",
    "# Load the extracted texts (assuming you have a dataframe named 'data')\n",
    "extracted_texts = data['Extracted_Text']  # Assuming 'data' is loaded with your text data\n",
    "\n",
    "# Calculate positive and negative scores, and additional derived variables for each extracted text\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "avg_sentence_lengths = []\n",
    "percentage_complex_words_list = []\n",
    "fog_indices = []\n",
    "avg_words_per_sentence_list = []\n",
    "syllables_per_word_list = []\n",
    "personal_pronouns_list = []\n",
    "avg_word_lengths = []\n",
    "\n",
    "for text in extracted_texts:\n",
    "    pos_score, neg_score = calculate_scores(text, positive_words, negative_words)\n",
    "    positive_scores.append(pos_score)\n",
    "    negative_scores.append(neg_score)\n",
    "    avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, syllables_per_word, personal_pronouns, avg_word_length = analyze_readability(text)\n",
    "    avg_sentence_lengths.append(avg_sentence_length)\n",
    "    percentage_complex_words_list.append(percentage_complex_words)\n",
    "    fog_indices.append(fog_index)\n",
    "    avg_words_per_sentence_list.append(avg_words_per_sentence)\n",
    "    syllables_per_word_list.append(syllables_per_word)\n",
    "    personal_pronouns_list.append(personal_pronouns)\n",
    "    avg_word_lengths.append(avg_word_length)\n",
    "\n",
    "# Add calculated derived variables to the dataframe\n",
    "data['Positive_Score'] = positive_scores\n",
    "data['Negative_Score'] = negative_scores\n",
    "data['Polarity_Score'] = [calculate_polarity_score(pos, neg) for pos, neg in zip(positive_scores, negative_scores)]\n",
    "data['Subjectivity_Score'] = [calculate_subjectivity_score(pos, neg, len(word_tokenize(text.lower()))) for pos, neg, text in zip(positive_scores, negative_scores, extracted_texts)]\n",
    "data['Avg_Sentence_Length'] = avg_sentence_lengths\n",
    "data['Percentage_of_Complex_Words'] = percentage_complex_words_list\n",
    "data['FOG_Index'] = fog_indices\n",
    "data['Avg_Words_Per_Sentence'] = avg_words_per_sentence_list\n",
    "data['Syllables_Per_Word'] = syllables_per_word_list\n",
    "data['Personal_Pronouns'] = personal_pronouns_list\n",
    "data['Avg_Word_Length'] = avg_word_lengths\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a07c7616-0cb2-4a07-ba56-7d41c151c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Extracted_Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c07618d-d424-4b1b-b73b-b8893c7c6d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>Subjectivity_Score</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_of_Complex_Words</th>\n",
       "      <th>FOG_Index</th>\n",
       "      <th>Avg_Words_Per_Sentence</th>\n",
       "      <th>Syllables_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "      <th>Avg_Word_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>11</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>208.0</td>\n",
       "      <td>34.615385</td>\n",
       "      <td>97.046154</td>\n",
       "      <td>208.0</td>\n",
       "      <td>2.245192</td>\n",
       "      <td>1</td>\n",
       "      <td>6.701923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>46</td>\n",
       "      <td>-25</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>0.032659</td>\n",
       "      <td>621.0</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>264.696296</td>\n",
       "      <td>621.0</td>\n",
       "      <td>2.400966</td>\n",
       "      <td>1</td>\n",
       "      <td>7.450886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>33</td>\n",
       "      <td>-19</td>\n",
       "      <td>3.714285</td>\n",
       "      <td>0.029979</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48.672566</td>\n",
       "      <td>200.269027</td>\n",
       "      <td>452.0</td>\n",
       "      <td>2.581858</td>\n",
       "      <td>1</td>\n",
       "      <td>7.809735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>33</td>\n",
       "      <td>-64</td>\n",
       "      <td>-3.129032</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>514.0</td>\n",
       "      <td>43.579767</td>\n",
       "      <td>223.031907</td>\n",
       "      <td>514.0</td>\n",
       "      <td>2.503891</td>\n",
       "      <td>1</td>\n",
       "      <td>7.776265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>25</td>\n",
       "      <td>-10</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>381.0</td>\n",
       "      <td>42.257218</td>\n",
       "      <td>169.302887</td>\n",
       "      <td>381.0</td>\n",
       "      <td>2.433071</td>\n",
       "      <td>1</td>\n",
       "      <td>7.388451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   Positive_Score  Negative_Score  Polarity_Score  Subjectivity_Score  \\\n",
       "0              11              -3        1.750000            0.037037   \n",
       "1              46             -25        3.380952            0.032659   \n",
       "2              33             -19        3.714285            0.029979   \n",
       "3              33             -64       -3.129032           -0.058491   \n",
       "4              25             -10        2.333333            0.037879   \n",
       "\n",
       "   Avg_Sentence_Length  Percentage_of_Complex_Words   FOG_Index  \\\n",
       "0                208.0                    34.615385   97.046154   \n",
       "1                621.0                    40.740741  264.696296   \n",
       "2                452.0                    48.672566  200.269027   \n",
       "3                514.0                    43.579767  223.031907   \n",
       "4                381.0                    42.257218  169.302887   \n",
       "\n",
       "   Avg_Words_Per_Sentence  Syllables_Per_Word  Personal_Pronouns  \\\n",
       "0                   208.0            2.245192                  1   \n",
       "1                   621.0            2.400966                  1   \n",
       "2                   452.0            2.581858                  1   \n",
       "3                   514.0            2.503891                  1   \n",
       "4                   381.0            2.433071                  1   \n",
       "\n",
       "   Avg_Word_Length  \n",
       "0         6.701923  \n",
       "1         7.450886  \n",
       "2         7.809735  \n",
       "3         7.776265  \n",
       "4         7.388451  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "736d7719-b523-494d-9c31-4d88051d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Output Data Structure.xlsx\"\n",
    "data.to_excel(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
